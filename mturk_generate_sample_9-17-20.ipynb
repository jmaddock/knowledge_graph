{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create samples for mturk validation\n",
    "\n",
    "**Author:** Jim Maddock\n",
    "\n",
    "**Last Updated:** 9-30-20\n",
    "\n",
    "**Description:**\n",
    "* 45 minutes per 100 tasks (~ 30 seconds per task), 5 minutes per job\n",
    "* \\\\$15 hour = \\\\$1.25 per job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "pd.set_option(\"display.min_rows\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILEPATH = '/Users/klogg/research_data/wmf_knowledge_graph/wiki_1300_72k_7-14-20/wiki_1300_72k_logs_5434743174174776460_taxo.json'\n",
    "\n",
    "def loadClusters(filepath):\n",
    "\n",
    "    with open(filepath) as json_file:\n",
    "        cluster_65 = json.load(json_file)\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for i, cluster in enumerate(cluster_65):\n",
    "        chunk = []\n",
    "        for article in cluster_65[cluster]['items']:\n",
    "            row = {\n",
    "                'label':cluster_65[cluster]['label'],\n",
    "                'cluster':cluster_65[cluster]['cluster'],\n",
    "                'w':article['w'],\n",
    "                'title':article['title']\n",
    "            }\n",
    "            chunk.append(row)\n",
    "        df = df.append(pd.DataFrame(chunk))\n",
    "        #print('finished cluster: {0}'.format(i))\n",
    "\n",
    "    df['title'] = df['title'].apply(lambda x: x.replace(' ','_'))\n",
    "    df['cluster'] = pd.to_numeric(df['cluster'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = loadClusters(FILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max articles per topic: 268\n",
      "min articles per topic: 4\n"
     ]
    }
   ],
   "source": [
    "print('max articles per topic: {0}'.format(df.groupby('cluster').size().max()))\n",
    "print('min articles per topic: {0}'.format(df.groupby('cluster').size().min()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BINS = 13\n",
    "SAMPLES_PER_BIN = 10\n",
    "SAMPLE_FILEPATH = '/Users/klogg/research_data/wmf_knowledge_graph/mturk/sample_10-29-20.csv'\n",
    "\n",
    "def getSample(df):\n",
    "    out, bins = pd.qcut(df.groupby('cluster').size(),NUM_BINS, retbins=True)\n",
    "    clusters_to_sample = out.to_frame('bin').groupby('bin').apply(lambda x: x.sample(SAMPLES_PER_BIN)).index.get_level_values('cluster').values\n",
    "    sample = df.loc[df['cluster'].isin(clusters_to_sample)]\n",
    "    return sample\n",
    "\n",
    "sample = getSample(df)\n",
    "sample.to_csv(SAMPLE_FILEPATH,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample clusters: 130\n",
      "sample articles: 7205\n",
      "number of tasks: 1441.0\n",
      "number of jobs: 432.29999999999995\n",
      "total cost: $648.45\n"
     ]
    }
   ],
   "source": [
    "ARTICLES_PER_TASK = 5\n",
    "TASKS_PER_JOB = 10\n",
    "CODERS_PER_JOB = 3\n",
    "PAY_RATE = 1.25\n",
    "\n",
    "tasks = len(sample) / ARTICLES_PER_TASK\n",
    "jobs = (tasks / TASKS_PER_JOB) * CODERS_PER_JOB\n",
    "cost = (jobs * PAY_RATE) + (jobs * PAY_RATE * .2)\n",
    "\n",
    "print('sample clusters: {0}'.format(sample['cluster'].nunique()))\n",
    "print('sample articles: {0}'.format(len(sample)))\n",
    "print('number of tasks: {0}'.format(tasks))\n",
    "print('number of jobs: {0}'.format(jobs))\n",
    "print('total cost: ${0}'.format(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/klogg/dev/knowledge_graph/venv/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "TASKS_FILEPATH = '/Users/klogg/research_data/wmf_knowledge_graph/mturk/tasks_10-29-20.csv'\n",
    "\n",
    "sample['rand'] = np.random.rand(len(sample))\n",
    "sample = sample.sort_values(['cluster','rand'])\n",
    "\n",
    "task = 0\n",
    "task_list = []\n",
    "chunk = []\n",
    "\n",
    "for i, article in sample.iterrows():\n",
    "    \n",
    "    if len(chunk) > 0 and article['cluster'] != chunk[-1]['cluster']:\n",
    "        chunk = []\n",
    "        task += 1\n",
    "    \n",
    "    row = {\n",
    "        'label':article['label'],\n",
    "        'cluster':article['cluster'],\n",
    "        'title':article['title'],\n",
    "        'task':task,\n",
    "        'imposter':False\n",
    "    }\n",
    "    chunk.append(row)\n",
    "    \n",
    "    if len(chunk) % ARTICLES_PER_TASK == 0:\n",
    "        imposter = df.loc[df['cluster'] != article['cluster']].sample().to_dict('records')[0]\n",
    "        row = {\n",
    "            'label':imposter['label'],\n",
    "            'cluster':imposter['cluster'],\n",
    "            'title':imposter['title'],\n",
    "            'task':task,\n",
    "            'imposter':True\n",
    "        }\n",
    "        task_list.extend(chunk)\n",
    "        task_list.append(row)\n",
    "        chunk = []\n",
    "        task += 1\n",
    "\n",
    "task_df = pd.DataFrame(task_list)\n",
    "\n",
    "task_df.to_csv(TASKS_FILEPATH,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real percent true: 0.9022302009252227\n",
      "simulated mean percent true: 0.9018384615384617\n",
      "simulated 95% confidence interval: (0.9002132851867921, 0.9034636378901313)\n"
     ]
    }
   ],
   "source": [
    "TRIALS = 1000\n",
    "n = int(len(df)/ARTICLES_PER_TASK)\n",
    "sample_n = task_df['task'].nunique()\n",
    "\n",
    "rand_df = pd.DataFrame({'rand':np.random.choice(a=[False, True], size=(n,), p=[.1,.9])})\n",
    "\n",
    "sample_means = []\n",
    "for x in range(TRIALS):\n",
    "    sample_rand_df = rand_df.sample(sample['cluster'].nunique())\n",
    "    sample_means.append(len(sample_rand_df.loc[sample_rand_df['rand'] == True])/len(sample_rand_df))\n",
    "    \n",
    "confidence_level = 0.95\n",
    "degrees_freedom = len(sample_means) - 1\n",
    "sample_mean = np.mean(sample_means)\n",
    "sample_standard_error = stats.sem(sample_means)\n",
    "confidence_interval = stats.t.interval(confidence_level, degrees_freedom, sample_mean, sample_standard_error)\n",
    "\n",
    "confidence_interval\n",
    "\n",
    "print('real percent true: {0}'.format(len(rand_df.loc[rand_df['rand'] == True])/len(rand_df)))\n",
    "print('simulated mean percent true: {0}'.format(sample_mean))\n",
    "print('simulated 95% confidence interval: {0}'.format(confidence_interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_title(title):\n",
    "    title = title.replace(\"'\",\"\")\n",
    "    return title\n",
    "\n",
    "NUM_JOBS = None\n",
    "NUM_CHUNKS = 10\n",
    "JOBS_FILEPATH = '/Users/klogg/research_data/wmf_knowledge_graph/mturk/jobs_{0}_10-19-20.csv'\n",
    "\n",
    "def stackForMturk(task_df, num_jobs=None, num_chunks=0):\n",
    "    \n",
    "    groups = [task_df for _, task_df in task_df.groupby('task')]\n",
    "    random.shuffle(groups)\n",
    "    task_df = pd.concat(groups).reset_index(drop=True)\n",
    "\n",
    "    count = 0\n",
    "    row = {}\n",
    "    chunk = []\n",
    "    task_list = []\n",
    "    imposter_map = {}\n",
    "\n",
    "    for i, article in task_df.iterrows():\n",
    "        title = format_title(article['title'])\n",
    "        q_num = count % (ARTICLES_PER_TASK + 1)\n",
    "        block_num = int(count/(ARTICLES_PER_TASK + 1)) % TASKS_PER_JOB\n",
    "        key = 'val_{0}_{1}'.format(block_num,q_num)\n",
    "        row[key] = title\n",
    "        if article['task'] not in task_list:\n",
    "            task_list.append(article['task'])\n",
    "        if article['imposter'] == True:\n",
    "            imposter_map[article['task']] = title\n",
    "        if block_num == TASKS_PER_JOB - 1 and q_num == ARTICLES_PER_TASK:\n",
    "            row['task_batch'] = int(count/(ARTICLES_PER_TASK + 1))\n",
    "            row['task_list'] = task_list\n",
    "            row['imposter_map'] = imposter_map\n",
    "            chunk.append(row)\n",
    "            row = {}\n",
    "            task_list = []\n",
    "            imposter_map = {}\n",
    "        count += 1\n",
    "\n",
    "    mturk_df = pd.DataFrame(chunk)\n",
    "    if num_jobs:\n",
    "        mturk_df = mturk_df.sample(num_jobs)\n",
    "    else:\n",
    "        mturk_df = mturk_df.sample(frac=1)\n",
    "\n",
    "    df_list = np.array_split(mturk_df, num_chunks)\n",
    "    for i, chunk in enumerate(df_list):\n",
    "        chunk.to_csv(JOBS_FILEPATH.format(i),index=False)\n",
    "        \n",
    "    return mturk_df\n",
    "\n",
    "mturk_df = stackForMturk(task_df=task_df,\n",
    "                         num_jobs=NUM_JOBS,\n",
    "                         num_chunks=NUM_CHUNKS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'${task_0}': ['${val_0_0}',\n",
       "  '${val_0_1}',\n",
       "  '${val_0_2}',\n",
       "  '${val_0_3}',\n",
       "  '${val_0_4}',\n",
       "  '${val_0_5}'],\n",
       " '${task_1}': ['${val_1_0}',\n",
       "  '${val_1_1}',\n",
       "  '${val_1_2}',\n",
       "  '${val_1_3}',\n",
       "  '${val_1_4}',\n",
       "  '${val_1_5}'],\n",
       " '${task_2}': ['${val_2_0}',\n",
       "  '${val_2_1}',\n",
       "  '${val_2_2}',\n",
       "  '${val_2_3}',\n",
       "  '${val_2_4}',\n",
       "  '${val_2_5}'],\n",
       " '${task_3}': ['${val_3_0}',\n",
       "  '${val_3_1}',\n",
       "  '${val_3_2}',\n",
       "  '${val_3_3}',\n",
       "  '${val_3_4}',\n",
       "  '${val_3_5}'],\n",
       " '${task_4}': ['${val_4_0}',\n",
       "  '${val_4_1}',\n",
       "  '${val_4_2}',\n",
       "  '${val_4_3}',\n",
       "  '${val_4_4}',\n",
       "  '${val_4_5}'],\n",
       " '${task_5}': ['${val_5_0}',\n",
       "  '${val_5_1}',\n",
       "  '${val_5_2}',\n",
       "  '${val_5_3}',\n",
       "  '${val_5_4}',\n",
       "  '${val_5_5}'],\n",
       " '${task_6}': ['${val_6_0}',\n",
       "  '${val_6_1}',\n",
       "  '${val_6_2}',\n",
       "  '${val_6_3}',\n",
       "  '${val_6_4}',\n",
       "  '${val_6_5}'],\n",
       " '${task_7}': ['${val_7_0}',\n",
       "  '${val_7_1}',\n",
       "  '${val_7_2}',\n",
       "  '${val_7_3}',\n",
       "  '${val_7_4}',\n",
       "  '${val_7_5}'],\n",
       " '${task_8}': ['${val_8_0}',\n",
       "  '${val_8_1}',\n",
       "  '${val_8_2}',\n",
       "  '${val_8_3}',\n",
       "  '${val_8_4}',\n",
       "  '${val_8_5}'],\n",
       " '${task_9}': ['${val_9_0}',\n",
       "  '${val_9_1}',\n",
       "  '${val_9_2}',\n",
       "  '${val_9_3}',\n",
       "  '${val_9_4}',\n",
       "  '${val_9_5}']}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "for i in range(0,10):\n",
    "    l = []\n",
    "    for j in range(0,6):\n",
    "        l.append('${val_'+str(i)+'_'+str(j)+'}')\n",
    "    key = '${task_'+str(i)+'}'\n",
    "    d[key] = l\n",
    "            \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS_FILEPATH = '/Users/klogg/research_data/wmf_knowledge_graph/mturk/tasks_10-29-20.csv'\n",
    "task_df.to_csv(TASKS_FILEPATH,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
